{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 257M\r\n",
      "drwxrwxr-x  2 zyt zyt 4.0K Nov  2 15:36 .\r\n",
      "drwxrwxr-x 10 zyt zyt 4.0K Nov  9 22:11 ..\r\n",
      "-rw-rw-r--  1 zyt zyt 5.9M Sep 28 11:01 df_test_corpus.tar.bz2\r\n",
      "-rw-rw-r--  1 zyt zyt 6.5M Nov  9 17:43 df_test_line.tar.bz2\r\n",
      "-rw-rw-r--  1 zyt zyt 106M Sep 28 11:01 df_train_corpus.tar.bz2\r\n",
      "-rw-rw-r--  1 zyt zyt 126M Nov  9 17:43 df_train_line.tar.bz2\r\n",
      "-rw-rw-r--  1 zyt zyt 6.4M Sep 28 11:01 df_valid_corpus.tar.bz2\r\n",
      "-rw-rw-r--  1 zyt zyt 7.3M Nov  9 17:43 df_valid_line.tar.bz2\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/ -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'datasets/'\n",
    "df_train = pd.read_pickle(os.path.join(save_path, 'df_train_line.tar.bz2'))\n",
    "df_test = pd.read_pickle(os.path.join(save_path, 'df_test_line.tar.bz2'))\n",
    "df_valid = pd.read_pickle(os.path.join(save_path, 'df_valid_line.tar.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 248,  448,   69, ...,    0,    0,    0],\n",
       "       [  17,   14,  115, ...,    0,    0,    0],\n",
       "       [ 341, 4332,    9, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [2282,  955,   11, ...,    0,    0,    0],\n",
       "       [ 269,   11, 1379, ...,    0,    0,    0],\n",
       "       [1162, 1234,    5, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.CodeTokenizer import CodeSplitTokenizer\n",
    "tokenizer = CodeSplitTokenizer('./vocabs/split_keyword_vocab50000.txt')\n",
    "valid_data = tokenizer.from_lines_to_token_input(df_valid['data'])\n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557254, 494331)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_valid), len(valid_data)\n",
    "# token的过程可能减少数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(lines, labels):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lines, labels))\n",
    "    ds = ds.shuffle(1000000).repeat()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_code = tokenizer.from_lines_to_token_input(df_valid[df_valid['label']==0]['data'])\n",
    "val_docs = tokenizer.from_lines_to_token_input(df_valid[df_valid['label']==1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(315229, 179102)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_code), len(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_code_ds = make_ds(val_code, np.zeros((len(val_code))))\n",
    "val_docs_ds = make_ds(val_docs, np.ones((len(val_docs))))\n",
    "val_ds = tf.data.experimental.sample_from_datasets([val_code_ds, val_docs_ds], weights=[0.5,0.5])\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1974  1589    11  1589    10   337    82    11     3    10    80  1046\n",
      "     11     3    10    96   834    11     3    10     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1   132   131  3708   369   132    29    49  7279   839    32  1081\n",
      "   4913     9     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12  3431    12   293     7    56     8     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   60   569  2763 43797   674     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 6659    32    32   131    32    32  1332   448 12181 29477  1999  6039\n",
      "    729   581    19     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  631  1332   415  3638  1332   190  6370  1999 12328     9     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [43612     1    11 15754   152     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    9     9  2733    12    12  1366     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1    11     1    14     4    15    14     4    15    10     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   25   156    11     4    10     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 6389  1082    11   370     9   152     7   445   482  4229     8     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 2655    11     4    57     7     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1131  4093    11   370     9   399     9  2129     7    44     7   777\n",
      "    343     8     8     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 8226     1   549  1234  7140  2005   381  2158   227 17274     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   18   319  1305     7    13    10  1305    31     8    12     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   53   369   942  1763    10  9554  1234  1999  9844    16  1332     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16    13     9   383   307    12     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  126   258 10117  1999   132   131    67 10117 27229     1     7  1638\n",
      "  25877   131    10    49  1234  1999  1295  6581  1081   132   131     8\n",
      "      0     0     0     0     0     0]\n",
      " [ 1294     9     1  2039     7     4     9    38     7     7   528   258\n",
      "     10    35   258     8     8    10   137   482   304    11    42     8\n",
      "      0     0     0     0     0     0]\n",
      " [    1   369  1332  2323  2658    32  6935 25043 16976   497  1332  1125\n",
      "   2073    60  6935    48    12     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1   369 17553   645     3 10398    67  5705    10    14 18282    33\n",
      "    707    15     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   22   132    19    13     9   397    12     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  690     9    69     7     4     9    38     7     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1661    60   431    10  2176     7   903    10    73     8    10   106\n",
      "      9   327     9    22   132   229     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 6784  1332  8156    63   195     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    9     9    14     3    15     1    10     1   591     9     1  5220\n",
      "    369     1    49     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   93    69    11    13     9    23   191   171     7   549    21    10\n",
      "    191    21    10     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   17    14    20    15     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1  6755    11  5432  6755    32     1     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1234   219    49   326 16133     9  2096   729   645  1332  1253   369\n",
      "    132   201   837     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   22    79    19   206     9  1365    12     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12   292   231    12    32  1082   369  1992    25   521   231     7\n",
      "      3     8  5036   231     7   432     8     0     0     0     0     0\n",
      "      0     0     0     0     0     0]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1.]\n",
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "# val_ds = tf.data.Dataset.from_tensor_slices((valid_data, df_valid['label']))\n",
    "for line, label in val_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())\n",
    "    print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他集合编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e7e061be3fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_lines_to_token_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_lines_to_token_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/Lab/codeclf/utils/CodeTokenizer.py\u001b[0m in \u001b[0;36mfrom_lines_to_token_input\u001b[0;34m(self, lines, threshold, maxlen)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mint_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_row_to_token_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/Lab/codeclf/utils/CodeTokenizer.py\u001b[0m in \u001b[0;36mfrom_row_to_token_id\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_row_to_token_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;34m\"\"\"把一行代码转成token\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mtokens_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MyCode/Lab/codeclf/utils/Utils.py\u001b[0m in \u001b[0;36mcreate_generator\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 生成器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = tokenizer.from_lines_to_token_input(df_train['data'])\n",
    "print(len(df_train), len(train_data))\n",
    "test_data = tokenizer.from_lines_to_token_input(df_test['data'])\n",
    "print(len(df_test), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code = tokenizer.from_lines_to_token_input(df_train[df_train['label']==0]['data'])\n",
    "train_docs = tokenizer.from_lines_to_token_input(df_train[df_train['label']==1]['data'])\n",
    "test_code = tokenizer.from_lines_to_token_input(df_test[df_test['label']==0]['data'])\n",
    "test_docs = tokenizer.from_lines_to_token_input(df_test[df_test['label']==1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398611, 2756143, 287562, 154148)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_code), len(train_docs), len(test_code), len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3468    43  1402  7279    24    39  2494  6365    35    28   691    19\n",
      "   1332  1419     9     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    4    49    50     9   493     7   166   560     9   454    14    45\n",
      "     15     8   147     1    33    50     9   493     7   166   560     9\n",
      "    454    14    45    15     8     8]\n",
      " [   16   634    24    19   129    49   303   691   107     4    12     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12   292   143    12  1186   369  1332   686   997     3    32     3\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1   714     7  2870  1512  1372     8     1    19   325    60  1118\n",
      "   9238     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16    97     9   143   150   107     3    12     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12    39    12    12   165    12     5     1     9   112     9  3251\n",
      "     36     5     7    16    24  4794     8     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   18    86   236     7    13     8    12     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 3487     9   397    14     4    15    11     4     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   22  1332    90    60  1222     9  7045  6031    16     5     5     5\n",
      "    665     5     5    28     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1655  2108   218     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1   261    11     3     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 5549    11     4     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16   242     9   167    30     4    12     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  441  1332  3745    16  1402  2822  4039     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 2166  1962    11     4    16   384    30     4    27    20     5     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16     4    19    13     9    66    14     4    15    12     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12  9521    12   159    16 12518    28   132   246  2877  1109   122\n",
      "   6031   917     9     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  225    11   228    79    49  3922     9   225    14  3162    15     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1521   677  2839  6278     9     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1007    31    11  1007    31    10     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   13     9     1    11    65     7     1    11     1    10     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  114    63    11    99    91  4352     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   99  1443  1387    11     7  1430     9    99   169    33     3     8\n",
      "      7    33  1443  1387     8     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1174   108    95     9 36539    63    63   180 24218    63    63     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  527    11    13     9    66     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 1174   108     6     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  129     9    52     7   243     8     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  335    11    43   301    48    22 11248     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 4552    11   982    25    14     4    15     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  272    11   272     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  313   214    14   465 14431    15    10     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]]\n",
      "[1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      "0.40625\n"
     ]
    }
   ],
   "source": [
    "train_code_ds = make_ds(train_code, np.zeros((len(train_code))))\n",
    "train_docs_ds = make_ds(train_docs, np.ones((len(train_docs))))\n",
    "train_ds = tf.data.experimental.sample_from_datasets([train_code_ds, train_docs_ds], weights=[0.5,0.5])\n",
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((valid_data, df_valid['label']))\n",
    "for line, label in train_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())\n",
    "    print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1313  3137     9 14814 18408     7   166    29  1726    10   104   907\n",
      "      8     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   61   159    12     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   17    13     9   367   134  2717   122   458    69     7   134    31\n",
      "     10    55    47     8     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   65 29477  1999  1295    60   163   132   902    49    53 29477  1999\n",
      "   1295    60     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    5   287    26     4  1736     5     5   287    26     4   438     6\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 6960   170     1   132  5656  3254    90   227   140     9   482   347\n",
      "   2763     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16   666   125    28    20    12     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 2723    19 14075    48     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   60  1332     5     5     5   172     5     5  2820   303  1332   160\n",
      "    670     9     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   18    23     7    13     8    12     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 8428  1088   151  7196    49  2016  1332   755   523   369  1332     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  874  3898    12    81    10     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 8428  1088  4600  2008     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  484    14     4    15    11    13     9   177   225    14     4    15\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  103   485    11   953    33  2700    33   724     9   947     7   738\n",
      "      8     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  102    36    12    16  1332   154  7279    24  3012  1332   806   192\n",
      "    210    67  1332     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12  3431    12     1     9   226     9  2681   251     9 15163     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   60   747    48     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12   292   626  1746   281   516    12     1   132   281   516   952\n",
      "     10    49   729  6370    17   951  1638   174  1332   516     9     0\n",
      "      0     0     0     0     0     0]\n",
      " [   13     9   999     7     4    10    20    10  2921     8     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  128  3993   710    11   240     9    82     7  3993   710     8     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 8226     5     5 22428     5   344  1754  1999  2763     5     5    81\n",
      "      5    67     5     5    20     5     9     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   55  1043    47    12  1537     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   75     9   124     7    13     9   148    53     8     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   21    11  6126  4430  2752    14  4087    15     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   22 13454    19    13     9  8269    12     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   23  4452  1359    11   361     9   278     7     4     8     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [  100   196     7   370     9  1074     8    12   436     3   532  1460\n",
      "   1574     5     5     5     3     5     5    32   152  6031    28  1332\n",
      "    100     0     0     0     0     0]\n",
      " [ 1695  2591  4889 21044    95  1332   195     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   16  8227    19  1866    12     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   12   292    56   252    12     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 4753    85  4453   785     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]]\n",
      "[0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "test_code_ds = make_ds(test_code, np.zeros((len(test_code))))\n",
    "test_docs_ds = make_ds(test_docs, np.ones((len(test_docs))))\n",
    "test_ds = tf.data.experimental.sample_from_datasets([test_code_ds, test_docs_ds], weights=[0.5,0.5])\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(2)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((valid_data, df_valid['label']))\n",
    "for line, label in train_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())\n",
    "    print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: b'remove directory info only', label: 1.0\n",
      "line: b'return [cls.of_structs(x.value, y.value)', label: 0.0\n",
      "line: b'function below.', label: 1.0\n",
      "line: b'elif I_ord == 2:', label: 0.0\n",
      "line: b'for t in range(len(self.agent))]', label: 0.0\n"
     ]
    }
   ],
   "source": [
    "ds_valid = tf.data.Dataset.from_tensor_slices((df_valid['data'], df_valid['label']))\n",
    "for line, label in ds_valid.take(5):\n",
    "    print('line: {}, label: {}'.format(line, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((df_train['data'], df_train['label']))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((df_test['data'], df_test['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = df_train[df_train['label']==0]\n",
    "docs_df = df_train[df_train['label']==1]\n",
    "len(code_df), len(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'else:'\n",
      "0.0\n",
      "b'- redis'\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def make_ds(lines, labels):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lines, labels))\n",
    "    ds = ds.shuffle(1000000).repeat()\n",
    "    return ds\n",
    "\n",
    "code_ds = make_ds(code_df['data'], code_df['label'])\n",
    "docs_ds = make_ds(docs_df['data'], docs_df['label'])\n",
    "for line, label in code_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())\n",
    "for line, label in docs_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b\"'self' and 'other'\" b'if idx:'\n",
      " b'Redirect call to our setup() tap function.'\n",
      " b'Since we need to use this sometimes'\n",
      " b'dst_dir   -- Optional destination directory to write files to.  If not'\n",
      " b'status_delta[x] -= 1' b'define MAVLINK_MSG_ID_${id}_CRC ${crc_extra}'\n",
      " b'----------' b'excs.append(straceback())' b'build payload'\n",
      " b\"'equipotential field lines, in turn alters '\" b'else:'\n",
      " b'self._in_queue.put(inPacket)' b'err_logfile_writer.close()'\n",
      " b'Create the SCOOP module arguments parser.'\n",
      " b'tail = [x for x in services if x is not service]'\n",
      " b\"all cases. Finally, datagrepper turns the 'timestamp' field into a float, but it\"\n",
      " b\"raise TypeError('Impossible to initialise the object from an object of type {}'.format(type(f)))\"\n",
      " b'.. note:: If ``package_name`` is passed and refers to a namespace'\n",
      " b'A parallel algorithm for computing the thickness of 3D objects,'\n",
      " b'if mmi_mean[i] <= 7.0:' b'A block to declare self variables'\n",
      " b'TODO: maybe we need an explicit marker for \"end of stream\"'\n",
      " b'\\tsequences is higher than that, -1 is returned.'\n",
      " b'names (list): The names of the dags that have to terminate.'\n",
      " b\"r['RevisionDate'] = datetime.fromtimestamp(r['RevisionDate'])\"\n",
      " b'output_path)' b':type data: bytearray'\n",
      " b'path_spec (PathSpec): path specification.' b':param item:'\n",
      " b'print(\"\\\\n' b'Display the data of the given index']\n",
      "[1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "resampled_ds = tf.data.experimental.sample_from_datasets([code_ds, docs_ds], weights=[0.5,0.5])\n",
    "resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)\n",
    "for line, label in resampled_ds.take(1):\n",
    "    print(line.numpy())\n",
    "    print(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "for _, label in resampled_ds.take(1):\n",
    "    print(label.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50007"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=50007, input_length=30, output_dim=200),\n",
    "        tf.keras.layers.LSTM(200),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 205333/Unknown - 38419s 187ms/step - loss: 0.5648 - accuracy: 0.8683"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
